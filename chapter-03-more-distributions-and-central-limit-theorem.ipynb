{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d422a9ef",
   "metadata": {},
   "source": [
    "# 📊 Chapter 03: More Distributions and the Central Limit Theorem\n",
    "\n",
    "You'll learn about the binomial distribution for visualizing the probability of binary outcomes, and one of the most important distributions in statistics, the normal distribution. You'll see how distributions can be described by their shape, along with discovering the Poisson distribution and its role in calculating the probabilities of events occurring over time. You'll also gain an understanding of the central limit theorem!\n",
    "\n",
    "## 🎯 🔸 Binomial Distribution\n",
    "- **Used For**: Binary outcomes (e.g., coin flips, success/failure).\n",
    "- **Parameters**:\n",
    "  - `n`: number of independent trials  \n",
    "  - `p`: probability of success\n",
    "- **Expected Value**:  \n",
    "  $$\\text{Expected value} = n \\times p$$\n",
    "- **Key Point**: Requires **independent** events.\n",
    "- **Example Applications**:\n",
    "  - Clinical drug trials (effective or not)\n",
    "  - Sports betting (win or lose)\n",
    "\n",
    "## 📈 🔸 Normal Distribution\n",
    "- **Shape**: Symmetrical bell curve  \n",
    "- **Total Area Under Curve**: 1 \n",
    "- Curve never hits 0\n",
    "- Described by mean and standard deviation\n",
    "- **Empirical Rule**:\n",
    "  - 68% within 1 standard deviation  \n",
    "  - 95% within 2 standard deviations  \n",
    "  - 99.7% within 3 standard deviations\n",
    "- **Importance**:\n",
    "  - Many real-world datasets follow this pattern.\n",
    "  - Essential for hypothesis testing and many other statistical methods.\n",
    "\n",
    "### 🔹 ⚖️ Skewness\n",
    "- **Definition**: Measures the **asymmetry** of a distribution.\n",
    "- A distribution is:\n",
    "  - **Symmetric** when skewness ≈ 0\n",
    "  - **Positively skewed** (right tail longer) when skewness > 0\n",
    "\n",
    "  ![Positive Skewness](./assets/positive.png)\n",
    "  - **Negatively skewed** (left tail longer) when skewness < 0\n",
    "\n",
    "  ![Negative Skewness](./assets/negative.png)\n",
    "- **Example**: Income distribution is often positively skewed.\n",
    "\n",
    "### 🔹 🌋 Kurtosis\n",
    "- **Definition**: Describes the **\"tailedness\"** or the frequency of extreme values.\n",
    "- Types of kurtosis:\n",
    "  - **Mesokurtic** (normal shape): moderate tails (kurtosis ≈ 3)\n",
    "  - **Leptokurtic**: fatter tails (kurtosis > 3), more extreme values\n",
    "  - **Platykurtic**: thinner tails (kurtosis < 3), fewer extremes\n",
    "\n",
    "  ![Types of kurtosis](./assets/kurtosis.png)\n",
    "  \n",
    "- **Useful for**:\n",
    "  - Identifying potential outliers\n",
    "  - Understanding how volatile or \"peaky\" a distribution is\n",
    "\n",
    "## 🔄 🔸 Central Limit Theorem (CLT)\n",
    "- **Definition**: The sampling distribution of a statistic becomes closer to the normal distribution as the size of the sample increases.\n",
    "\n",
    "  ![Sampling Distribution](./assets/sampling.png)\n",
    "  \n",
    "### 🧪 Requirements:\n",
    "  - Random sampling\n",
    "  - Independent observations\n",
    "  - Sufficiently large sample size (n ≥ 30 is a decent rule of thumb)\n",
    "\n",
    "### 🔄 CLT Applies To:\n",
    "  - Means of samples\n",
    "  - Proportions of samples\n",
    "\n",
    "### 📐 What Gets Normalized?\n",
    "  - Not your raw data — that can be skewed, binary, or just plain strange.\n",
    "  - Instead, it’s the sampling distribution of the mean that becomes normal. That’s the keyword here: sampling distribution.\n",
    "  - It’s not about what you measure — it’s about how many times you measure and how consistently.\n",
    "\n",
    "### ✅ Benefits of the Central Limit Theorem (CLT)\n",
    "\n",
    "1. **Normality of Sampling Distributions**  \n",
    "   Sample means (or proportions) tend to form a **normal distribution**, even when the population distribution is not normal — as long as the sample size is large enough.\n",
    "\n",
    "2. **Enables Inferential Statistics**  \n",
    "   CLT underpins **confidence intervals** and **hypothesis testing**, allowing valid conclusions from sample data even without full population information.\n",
    "\n",
    "3. **Simplifies Analysis**  \n",
    "   Makes it possible to use **normal distribution-based methods** (z-scores, p-values, etc.) for real-world data that might otherwise be messy or complex.\n",
    "\n",
    "4. **Improves Estimation**  \n",
    "   Allows accurate estimation of **population parameters** (e.g., mean, proportion) from sample data, with quantifiable uncertainty.\n",
    "\n",
    "5. **Reduces Random Variability**  \n",
    "   Averages (sample means) are less variable than individual data points, offering **more consistent and reliable estimates**.\n",
    "\n",
    "6. **Works Across Many Distributions**  \n",
    "   The CLT applies broadly — to **binomial**, **Poisson**, **exponential**, and other distributions — making it a **universal tool** in statistics.\n",
    "\n",
    "7. **Effective with Modest Sample Sizes**  \n",
    "   Often works well with samples of **n ≥ 30**, which is practical for many real-world scenarios.\n",
    "\n",
    "## ⏱️ 🔸 Poisson Distribution\n",
    "- **Used For**: Counting the number of events in a fixed time or space.\n",
    "- **Assumptions**:\n",
    "  - Events are **independent**\n",
    "  - Events occur at a **constant average rate**\n",
    "- **Parameter**:\n",
    "  - $\\lambda$ (lambda): average number of events per interval  \n",
    "    $$\\lambda = \\text{expected value}$$\n",
    "- **Examples**:\n",
    "  - Website visits per day\n",
    "  - Animal adoptions per week\n",
    "  - Customer arrivals per hour\n",
    "- **Note**: The **CLT can also apply** to Poisson-distributed data with large samples.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
